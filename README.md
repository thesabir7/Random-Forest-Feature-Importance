# ðŸ“˜ Day 6: Random Forest & Feature Importance

## ðŸ“Œ What is a Random Forest?

A **Random Forest** is an ensemble of decision trees trained on different subsets of the data and/or features.  
Instead of relying on a single tree, it **averages** predictions (for regression) or takes a **majority vote** (for classification), reducing variance and improving generalization.

---

## âœ… Advantages
- **Higher accuracy** than a single Decision Tree
- Reduces **overfitting**
- Works well with high-dimensional data
- Provides **feature importance scores** for interpretability

---

## âš ï¸ Disadvantages
- Less interpretable than a single tree
- Requires more computation

---

## ðŸ§  Key Learning
> Ensemble methods like Random Forests can significantly boost performance by reducing variance.  
> Feature importance helps understand which inputs most influence predictions.

---

## ðŸ“Š Dataset
- **Titanic Dataset** (`train.csv` from Kaggle)
- Task: Predict passenger survival (`Survived`)

---
