# 📘 Day 6: Random Forest & Feature Importance

## 📌 What is a Random Forest?

A **Random Forest** is an ensemble of decision trees trained on different subsets of the data and/or features.  
Instead of relying on a single tree, it **averages** predictions (for regression) or takes a **majority vote** (for classification), reducing variance and improving generalization.

---

## ✅ Advantages
- **Higher accuracy** than a single Decision Tree
- Reduces **overfitting**
- Works well with high-dimensional data
- Provides **feature importance scores** for interpretability

---

## ⚠️ Disadvantages
- Less interpretable than a single tree
- Requires more computation

---

## 🧠 Key Learning
> Ensemble methods like Random Forests can significantly boost performance by reducing variance.  
> Feature importance helps understand which inputs most influence predictions.

---

## 📊 Dataset
- **Titanic Dataset** (`train.csv` from Kaggle)
- Task: Predict passenger survival (`Survived`)

---
